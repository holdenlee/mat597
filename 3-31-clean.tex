
{\color{blue}3/31/16: We are graduating from free energy to states of infinite systems.}

\subsection{Conditional expectation}

A point $\sigma\in \Omega$ is given by $\sigma=\{\sigma_x\}_{x\in \mathcal{G}}$. 
%$(\Om,\Si,\mu)$.

The conditional probability, conditioned on an event $B$ is $\mathbb{P}(A|B) = \frac{\mathbb{P}(A\cap B)}{\mathbb{P}(B)}$. 
%specify a $\si$-algebra.
%A partition of $\Om$ is a collection of disjoint sets $\Si_0$. If $A$ is a
We can define probability conditioned on a $\sigma$-algebra,
\be
\mathbb{P}(A|\Sigma_0)(\sigma).
\ee
%Conditional expectation on 
Thinking of $\Sigma_0$ as a partition (e.g. when it's finite),  it's the probability of $A$ given the element of the partition $\Sigma_0$ that $\sigma$ is in. We now define this more generally and formally.

\begin{prob*}[Regular conditional expectation value]
Let $(\Omega,\Sigma,\mu)$ be a probability space and $\Sigma_0\subseteq \Sigma$ (a sub $\sigma$-algebra). Then there is a unique map
%bounded function on the space $\Om$
\be
\mathbb{E}_{\Sigma_0}:L^{\infty}(\Omega,\Sigma) \to L^{\infty}(\Omega, \Sigma_0)
\ee
such that for all $f\in L^{\infty}(\Omega,\Sigma)$ and $g\in L^{\infty}(\Omega,\Sigma_0)$,
\begin{equation}\label{eq:cond-exp}
\int fg\,d\mu(\sigma) = \int g(\sigma) \mathbb{E}_{\Sigma_0}(f) \,\mu(d \sigma).
\end{equation}
\end{prob*}
Think of sub-$\sigma$-algebras as generalizations of partitions. Suppose for the moment that $\Sigma_0$ is a partition (the $\sigma$-algebra breaks up into atoms). What do I know about a point given partial information about it, namely, in which set of the partition it is in? I can get a conditional probability distribution by restricting to that set. To say that $g\in \mathcal{B}_{\Sigma_0}$ means that $g$ is constant on elements of the partition. The function $\mathbb{E}(f|\Sigma_0)(\sigma)$ is the mean value of $f$ restricted to the partition $\Sigma_0$. If $g\in \mathcal{B}_{\Sigma_0}$, to find the expectation of $fg$,~\eqref{eq:cond-exp} says that you can either integrate it directly, or take the average of $f$ on each set of the partition first and then integrate.

%certain subspace $L^2$.
%$\E_{\Si_0}$ is a projection
Furthermore, $\mathbb{E}_{\Sigma_0}$ acts as an orthogonal projection onto $L^2(\Omega,\Sigma_0)$. 
%conditional expectation to the idea that partial information corresponds to partition.
%conditional expectation of function conditioned on partial information, still function of $\si$.

%measurable with respect to partition if you know the set then you know the value
(In general things are more complicated. The $\sigma$-algebra corresponding to specifying $x$ is a vertical line. There is an uncountable number of ``atoms" each of zero measure. Here measurable means only depending on $x$. The discussion on partitions is for primarily for intuition.)

%We will use this to define equilibrium states.

%Box $\La$ with partial box $\La_0$.
Recall that a Gibbs equilibrium state in a finite volume $\Lambda$ with boundary conditions is a probability measure on $\Omega_{\Lambda}$ of the form $\frac{e^{-\beta H_{\Lambda}^{\left( \text{b.c.} \right)}(\sigma_\Lambda)}}{Z_{\Lambda}^{\left( \text{b.c.} \right)}}\rho_0(d\sigma_{\Lambda})$. 

Let $\Lambda_0\subseteq \Lambda$ and assume that the boundary condition term in $H$ does not involve $\sigma_{\Lambda_0}=\sigma|_{\Lambda_0}$. Denote 
\be\mathbb{E}(f|\Sigma_{\Lambda\backslash \Lambda_0})(\sigma) = \mathbb{E}(f|\sigma_{\Lambda_0^c}). \ee
%when $\Si_0$ is the $\si$-algebra generated by functions that are determined by $\si|_{\La_0}$.
%measurable with $\La_0$
Then (the DLR formula)
\begin{equation}\label{eq:dlr2}
\mathbb{E}(f|\sigma_{\Lambda_0^c}) = \int f(\sigma_{\Lambda_0}, \sigma_{\Lambda_0^c})  \frac{e^{-\beta H_{\Lambda}(\sigma_{\Lambda_0}| \sigma_{\Lambda_0^c})}}{Z}\rho_0(d\sigma_{\Lambda_0}).
\end{equation}
%any pair of finite volumes
%fluctuate
%summable series
This formula captures that for some equilibrium state in a huge volume, the local behavior is given by a canonical expression. 
In the expression the configuration in $\Lambda_0^c$ is frozen; we integrate over the spins inside. This formula makes sense even if $\Lambda$ is infinite. %in the infinite volume limit
This allows us to define Gibbs states for the infinite volume limit.

\begin{definition}
A probability measure $\mu$ on $(\Omega,\Sigma)$ is a \textbf{Gibbs state} for a Hamiltonian $H$ iff for any finite region $\Lambda_0$,
$
\mathbb{E}(f|\sigma_{\Lambda_0^c})
$ 
is given by the DLR formula~\eqref{eq:dlr2}.
\end{definition}
%entire configuration outside. 
%integrate interior with right probability distribution.
The Metropolis-Hastings algorithms gives a way to compute expectations by running dynamical system. Pick a site at random and redraw it with right conditional probability distribution. This is one way to numerically compute conditional expectation values. %After a flip, has right conditional distribution.
%stationary under that process.
%Refresh the finite volume with the right conditional volume. This is lso used as a basis for stochastic dynamics. 
The Gibbs state is stationary under these dynamics.

\begin{theorem}
Let $\Lambda_n\nearrow \mathcal{G}$, (e.g. $\mathcal{G}=\mathbb{Z}^d$) and $\mu_{\Lambda_n}$ be a sequence of finite volume 
\be
\mu_{\Lambda_n} = \frac{e^{-\beta H_{\Lambda_n}^{\left( \text{b.c.} \right)}(\sigma_{\Lambda_n})}}{Z}\rho_0(d\sigma_{\Lambda_n}).
\ee
If the weak limit
\be
w\lim_{n\to \infty} \mu_{\Lambda_n} = \mu
\ee
exists then the limiting measure is a Gibbs state.
\end{theorem}
\begin{definition}
We say $w\lim_{n\to \infty} \mu_{\Lambda_n} = \mu$ if 
\be
\lim_{n\to \infty} \int f\,d\mu_n(d\sigma)
\ee
exists for all local families, and this equals $\int f\,\mu(d\sigma)$.
%consistent, depend on spins in some finite subset.
%expectation values consistent.
%limit defines linear functional.
%such a probability measure for all gibbs state
\end{definition}
%When I was a kid I used to think infinite is more than any finite amount. With age I found the opposite is true.
For finite systems you can ask questions like ``what is the boundary"? In the infinite limit, it doesn't make sense to ask such questions.

%naturally defined for convergence of local functions.
%To prove the theorem, all we have to do is verify is the DLR formula.
The theorem follows from the DLR formula.

\begin{lemma}
The collection of Gibbs states is closed under convex combinations. I.e., if $\mu_0$ are Gibbs states for a common Hamiltonian, then so is 
\begin{enumerate}
\item
$\mu=(1-t)\mu_0+t\mu_1$, $0\le t\le 1$.
\item When $\nu$ is a probability measure ($\int \nu(d\alpha)=1$)
$\mu(-) = \int \mu_\alpha(-)\,\nu(d\alpha)$.
\end{enumerate}•
\end{lemma}

\begin{definition}
A Gibbs state is said to be a \textbf{pure} Gibbs measure if it does not admit a decomposition as a convex combination of distinct Gibbs states.
\end{definition}
%throughout the systems biased towards $+$
For example, consider the Ising model at $h=0$, $\beta>\beta_c$. We can take $+$ boundary conditions, $-$ boundary conditions, or free/periodic boundary conditions. We have that the Gibbs state with free/periodic boundary conditions is not a pure state:
\be
\mu = \lim_{\Lambda_n\nearrow \mathbb{Z}^d} \mu_{\Lambda_n}^{\left( \text{per} \right)} = \frac{1}{2} \mu^{(+)} + \frac{1}{2} \mu^{(-)}.
\ee
%what happens when you reach alpha centauri.
%finite: restrict attention to systems much smaller.

What if we mix $+$'s and $-$'s? The state of Gibbs states is large. 

\subsection{Symmetry and symmetry breaking}

\begin{definition}
A \textbf{symmetry} of a statistical mechanical systems is an invertible (measurable) mapping $T:\Omega\to \Omega$ which preserves $\rho_0$ and $H$ in the following sense:
\begin{enumerate}
\item
for all $A \in \Sigma$, $\rho_0(T^{-1}A)=\rho_0(A)$.
\item
$H(T\sigma)=H(\sigma)$, $\phi_{\Lambda}(T\sigma) = \phi_{\Lambda}(\sigma)$.
\end{enumerate}•
\end{definition}
\begin{definition}
A Gibbs state is said to exhibit \textbf{simple symmetry breaking} if $\mu$ is not invariant under one of the system's symmetries. %a symmetry $T$ of the system.
\end{definition}
\begin{example}
The Ising model at $h=0$ is symmetric with respect to $T(\sigma):=-\sigma$ %is $\rh_n(d\si)$, 
because $\rho_n(d\sigma)$ and $H=-\Sigma J_{x,y} \sigma_x\sigma_y$ are both invariant under $T(\sigma):=-\sigma$. 
\end{example}

All spins get equal weight makes sense for a finite system. What's the meaning for an infinite collection, when all configurations have 0 probability? We mean that for all $f$ local (depending on a finite number of spins), \be\int f(\sigma)\rho_0(d\sigma) = \int f(-\sigma) \rho_0(d\sigma).\ee We can talk about expectation values of functions, rather than probabilities of individual configurations.

Consider taking the limit
\be
\rho_0(d\sigma) \mapsto \lim_{\Lambda \nearrow \mathbb{Z}^d} \frac{e^{-\beta H(\sigma)}}{Z}\rho_0(d\sigma).
\ee
{\color{red}Before the limit, if we have free boundary conditions, this would be symmetric. If you put boundary conditions, this would not be symmetric. Does the asymmetry persist in the limit? It can't disappear; the limit is characterized by the local DLR equation.}
%product measure.
