\chapter{Introduction to statistical mechanics}

\blu{2-2-16}

\section{The equivalence principle}
\subsection{Configurations and ensembles}
One way to start is with the axioms of statistical mechanics. Instead I'll take a simple problem, see how it works, and present results in that context. There are simple problems that teach us a lot. The simplest is a lattice gas.

A lattice gas is a substrate where at each lattice site there may or may not be a particle. 

The \ivocab{configuration} is a function $n:\Z^d\to \{0,1\}$:
\[
n_x=\begin{cases}
1,&\text{$x$ is occupied},\\
0,&\text{$x$ is vacant}.
\end{cases}
\]
I'll use $L$ to denote the size of the box we are considering, and $\La\sub \Z^d$ to be a region (subset of the system). The \vocab{configuration space} is the space of possible $n$'s, $\{0,1\}^{\La}$.

We assume conversation in the number of particles, and that particles cannot overlap. %Suppose that every configuration gets equal weight. 
We make the \ivocab{equidistribution assumption}: every configuration with the same number of particles has equal probability. This gives rise to the \ivocab{microcanonical ensemble}. An ensemble is a probability measure with respect to which you do averages. We have
\[
\Pj(n_{\Om})= \fc{\one[\sum_{x\in \Om}n_x=N]}{Z}
\]
where $Z$ is a normalization constant.
Here, $\one[\text{cond}] := \begin{cases}
1,&\text{condition satisfied}\\
0,&\text{else}
\end{cases}$.

For every function $f:\Om\to \R$ assigning a real number to each configuration, define the \ivocab{microcanonical ensemble average} by
\[
\an{f}_{N,n}^{\text{Can}} = \fc{\sum_{n\in \Om} \one[n_x=N] f(n)}{\sum_{n\in \Om} \one[\sum n_x=N]}.
\]

Loosely, the \ivocab{equivalence principle} says that for any ``local function", the microcanonical average is approximately the grand canonical ensemble average
\[
\an{f}_{N,\La}^{\text{Can}}\approx \an{f}_{\mu, \La}^{\text{Gr.C}}
\] 
when we take $\mu = \fc{N}{|\La|}$.
%at suitable $\mu=\mu\pf{N}{n}$.

The \ivocab{grand canonical ensemble average} is defined as
\[
\an{f}_{\mu, \La}^{\text{Gr.C}} = \fc{\sum_{n\in \Om} e^{-\mu  \sum_{x\in \La}n_x}f(n)}{\sum_{n\in \Om}e^{-\mu \sum_{x\in \La} n_x}}
\]
(Later on we will omit superscripts where it is clear.)

\subsection{Equivalence principle: first proof}

Consider functions which depend only on a system $\La\subeq \wt{\La}$ of much smaller volume, $|\La|\ll |\wt{\La}|$. This is the sense in which the averages match up.

%sociological interest. 
The micro-canonical ensemble is draconian: the number of particles is prescribed, all other configurations get weight 0. %if don't fit bill get weight 0. 
In the grand canonical ensemble, each configuration contributes. There is a value of $\mu$ where the density is the same; at that value the local average of the draconian system is asymptotically the same at that of the more relaxed system. This $\approx$ becomes $=$ when you take the thermodynamic limit,
\bal
\wt\La & \to \Z^d\\
N & \to \iy\\
\fc{N}{|\wt \La|} & \to \rh.
\end{align*}

%If all that matters it the number of particles in $\La$, then we care about the induced distribution on $\La$. %What is the probability distribution 
What is the induced distribution of the micro-canonical ensemble on $\La$? Under the micro-canonical ensemble what is the probability that $n_{\La}$ (the restriction to $\La$) 
%n_{\Ga_n}
takes a particular value with $\sum_{n}n_x=k$? %If you specify a configuration in $\L$
We count the number of ways to complete the configuration in $\La^c=\wt\La\bs \La$:
\bal
\fc{|\set{n_{\La^c}}{\sum_{x\in \La^c}n_x=N-k}|}{Z}
\end{align*}
where $Z$ is a normalization constant.

The number of configurations of $M$ particles in volume $V$ is $\binom{V}{M} = \fc{V!}{M!(V-M)!}$. Using Stirling's approximation
\[
\ln (M!) = M(\ln M-1)(1+o(1)),
\]
%gymnastics of elementary type.
%Boltzmann grade in Vienna
%entropy
letting the \ivocab{entropy} $S$ be the logarithm of the number of configurations and %$S=k\ln W$,
\[
s(\rh) = -[\rh \ln \rh + (1-\rh)\ln (1-\rh)],
\]
we have
\[
\binom{V}{M} = \fc{V!}{M!(V-M)!} 
=: e^{S(M,V)}
\approx e^{Vs(\rh)}
\]
(do this calculation as an exercise).

Shannon also found such a formula for entropy.

This attains maximum of $\ln 2$ at $\rc2$ where it has quadratic behavior.

The implication is that if you slightly change the density, the number of configurations changes drastically. In physical substances $V$ may be $10^{23}$. The change would then be $e^{10^{23}\De s}$. In any average over configurations, only those at the peak contribute: ``winner takes all."

What is the probability of observing $k$ particles in the small box given $n$ in the big box?
\bal
\Pj(n_{\La}) &\approx \fc{e^{|\La^c|s\pf{N-k}{|\wt \La| - |\La|}}}{C}\\ %\quad \rh=\fc{N}{|\wt{\La}|}
%volume of completent times density.
\fc{N-k}{|\wt\La|-|\La|}&=\rh - \ub{\pa{\rh - \fc{N}{|\wt \La | - |\La|}}}{\to 0 \text{ independent of $k$}} -
\fc{k}{|\wt\La|-|\La|}\\
s\pf{N-k}{|\wt\La|-|\La|} & \approx s(\rh) - s'(\rh)\fc{k}{|\La^c|}
%particles here is small fraction of number of outside.
\end{align*}
Changing $k$ by a little bit affects how many particles are outside but not so much the density outside: the correction term is small. Hence for $\sum_{x\in \La}n_x$,
\[
\Pj(n_\La) \approx \fc{e^{|\La^c|s(\rh)}e^{-s'(\rh)k}}{Z}.
\]
$e^{|\La^c|s(\rh)}$ is a huge factor but it does not vary with $k$ so we can omit it. Writing out $Z$ as the sum of the numerators over all $n_\La$,  and letting $\mu = s'(\rh)$, we get that this equals
\[
=\fc{e^{-\mu k}}{\sum_{n'\in \Om_{\La}}e^{-\mu\sum_{x\in \La} n'_x}},
\]
which is the grand canonical ensemble average.
The rest of the system acts on the small system as a ``particle (heat) bath."
%\begin{thm}[Equivalence principle]
%Let $f:\Om \to \R$ be a function depending 
%\end{thm}

Here we used very explicit machinery, namely, the Stirling formula. We want a general expression that doesn't rely on the Stirling formula because in more complicated models, we will not have the luxury of using the Stirling approximation. We'll do this in the next section.

%Note we can also apply this method 
Then we will be able to consider models where there are more energy constraints. The general procedure is first make a list of energy constraint. There is a generalization of the equivalence principle where 
\begin{itemize}
\item
in the micro-canonical ensemble we average over configurations where the constraints have prescribed values. (We considered the special case where just the number of particles was prescribed.) %meet the constraint. All of them get equal weight.
\item 
in the grand canonical ensemble, we add an energy term to the exponential: $e^{-\mu N(n) - \be\cal E(n)}$ where $N(n)$ is the number of particles, $-\mu N(n)$ is the Gibbs factor, and $\cal E(n)$ the energy.
\end{itemize}
%Functions which depend on a small region, can be computed with Gibbs factors, $e^{-\mu N(n) - \be\cal E(n)}$ where $N(n)$ is the number of particles, $-\mu N(n)$ is the Gibbs factor, and $\cal E(n)$ the energy.

How can we construct an alternative method without the Stirling formula?

Define
\[
Z_{\wt \La} = \sum_{n\in \Om} e^{-\mu N(n)}.
\]
This can be easily computed without Stirling. From the value of this, you can learn the value of the entropy function:
\bal
&=\sum_{n\in \Om} \prod_{x\in \wt\La} e^{-\mu \one[n_x=1]}\\
&=\prod{x\in \wt{\La}}(1+e^{-\mu}) = (1+e^{-\mu})^{|\wt \La|}.
\end{align*}
However, 
\bal
Z_{\wt \La} &=\sum_{n\in \Om} e^{-\mu N(n)}\\
&= \sum_{K\in \N} e^{-\mu K}e^{V s\pf{K}{v}}\\
&= \sum_{K\in \N} e^{V(s\pf{K}{V}-\mu \fc{K}V)}.
\end{align*}
For each $k$ count how many configurations have that value of $k$. Here $S(V,K)=Vs\pf{K}{V}$.
Suppose we find it acceptable to say the system \emph{has} an entropy.

The maximal value of $\fc KV$ takes it all: this is
\[
=e^{V\max_{\rh\in [0,1]}[s(\rh)-\mu\rh]}.
\]
This is the \ivocab{Legendre transform} of the entropy.
Let
\beq{eq:s-leg}
s^*(\mu):= \max_\rh [s(\rh) - \mu\rh] = \ln (1+e^{-\mu})
\eeq
Next time we'll discuss how to derive from this expression the formula for $s(\rh)$, using the inverse Legendre transform.

``The elementary problems are the most precious, once you absorb them they are part of your makeup."